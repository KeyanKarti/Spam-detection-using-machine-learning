{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Count Vector Config Overview\n",
    "1. define stopwords\n",
    "2. define extent of lemmatization\n",
    "3. define tokenizer function (wrapper function for stopwords, lemmatization)\n",
    "4. define vector config (wrapper object for tokenizer function)\n",
    "5. define vectorization and matrix transformation\n",
    "\n",
    "Vector Config ToDo\n",
    "Apply more regorous corpus preprocessing:\n",
    "=> remove all non-english entries, experiement with ngram forms \n",
    "=> remove redundant tokens\n",
    "'''\n",
    "\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys\n",
    "\n",
    "# initialize constants\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def define_sw():\n",
    "    custom_stop_words = ['–', '\\u2019', 'u', '\\u201d', '\\u201d.','\\u201c', 'say', 'saying', 'sayings','says', 'us', 'un', '.\\\"', 'would','let', '.”', 'said', ',”', 'ax', 'max','b8f', 'g8v', 'a86', 'pl', '145', 'ld9', '0t','34u']\n",
    "    return set(stopwords.words('english') + custom_stop_words)\n",
    "\n",
    "\n",
    "def lemmatize(token, tag):\n",
    "    tag = {\n",
    "        'N': wordnet.NOUN,\n",
    "        'V': wordnet.VERB,\n",
    "        'R': wordnet.ADV,\n",
    "        'J': wordnet.ADJ\n",
    "    }.get(tag[0], wordnet.NOUN)\n",
    "\n",
    "    return lemmatizer.lemmatize(token, tag)\n",
    "\n",
    "\n",
    "def cab_tokenizer(document):\n",
    "    tokens = []\n",
    "    sw = define_sw()\n",
    "    punct = set(punctuation)\n",
    "\n",
    "    # split the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "        # tokenize each sentence\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "            # preprocess and remove unnecessary characters\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation, ignore token and continue\n",
    "            if all(char in punct for char in token):\n",
    "                continue\n",
    "\n",
    "            # If stopword, ignore token and continue\n",
    "            if token in sw:\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token and add back to the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "\n",
    "            # Append lemmatized token to list\n",
    "            tokens.append(lemma)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def generate_vector(params):\n",
    "    return CountVectorizer(tokenizer=cab_tokenizer, ngram_range=tuple(params['ngram_range']),\n",
    "                           min_df=params['min_doc_frequency'], max_df=params['max_doc_frequency'])\n",
    "\n",
    "\n",
    "def vectorize(tf_vectorizer, df):\n",
    "    # fit count vectorizer to supplied corpus, return term frequency matrix\n",
    "    df = df.reindex(columns=['tweet'])  # reindex on tweet\n",
    "\n",
    "    tf_matrix = tf_vectorizer.fit_transform(df['tweet'])\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "    return tf_matrix, tf_feature_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
