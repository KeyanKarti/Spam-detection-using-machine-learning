{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Dynamic Feature Generation Overview\n",
    "1. create LDA model object based upon a supplied term frequency corpus (vectorized corpus)\n",
    "2. generate Document/Topic and Topic/Word (unused currently) distributions based upon LDA model object\n",
    "3. calculate DT distribution entropy for all entries within term frequency matrix\n",
    "4. calculate LOSS and GOSS scores for all entries within term frequency matrix, utilizing DT distribution\n",
    "\n",
    "Dynamic Feature ToDo\n",
    "Optimize GOSS/LOSS functions\n",
    "'''\n",
    "\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import scipy as scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/KARTHIK/Desktop/spam detection using clusering and classification/code/scripts/util')\n",
    "import util\n",
    "from util import export_frame\n",
    "\n",
    "\n",
    "def create_lda(tf_matrix, params):\n",
    "    return LatentDirichletAllocation(n_components=params['lda_topics'], max_iter=params['iterations'],\n",
    "                                     learning_method='online', learning_offset=10,\n",
    "                                     random_state=0).fit(tf_matrix)\n",
    "\n",
    "\n",
    "def create_tw_dist(model):\n",
    "    # return normalized topic-word distribution\n",
    "    normTWDist = model.components_ / \\\n",
    "        model.components_.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    return normTWDist\n",
    "\n",
    "\n",
    "def create_dt_dist(model, tf_matrix):\n",
    "    # return normalized document-topic distribution\n",
    "    normDTDist = model.transform(tf_matrix)\n",
    "\n",
    "    return normDTDist\n",
    "\n",
    "\n",
    "def entropy_single(x):\n",
    "    # calculate entropy for a given sequence of values\n",
    "    return scipy.stats.entropy(x)\n",
    "\n",
    "\n",
    "def entropy_all(dt_dist):\n",
    "    # calculate entropy for an entire document-topic distribution\n",
    "    np_entropy = np.apply_along_axis(entropy_single, axis=1, arr=dt_dist)\n",
    "\n",
    "    return pd.DataFrame(np_entropy, columns=['dt_entropy'])\n",
    "\n",
    "\n",
    "def single_goss(topic_dist, i, k):\n",
    "    # calculate GOSS score for a single particular user/topic (i/k) combination\n",
    "\n",
    "    # 1.0 return mu(xk) for specific topic, sum topic probabilities for all\n",
    "    # users, average across all users\n",
    "    mu_xk = np.sum(topic_dist[:, k]) / topic_dist.shape[0]\n",
    "\n",
    "    # 2.0 GOSS equation numerator\n",
    "    goss_numerator = topic_dist[i, k] - mu_xk\n",
    "\n",
    "    # 3.0 for all users specific topic probability:\n",
    "    # - sum the squared difference of their relevant topic probability\n",
    "    # - find the square of this sum\n",
    "    goss_denominator = 0\n",
    "    for user_prob in topic_dist[:, k]:\n",
    "        goss_denominator += (user_prob - mu_xk) ** 2\n",
    "\n",
    "    # 3.1 find sqrt of goss_denominator\n",
    "    goss_denominator = sqrt(goss_denominator)\n",
    "\n",
    "    # 4.0 divide numerator/denominator to find final GOSS score for user/topic\n",
    "    # combination\n",
    "    return goss_numerator / goss_denominator\n",
    "\n",
    "\n",
    "def all_goss(topic_dist):\n",
    "    # calculate GOSS scores for a particular topic distribution\n",
    "    goss = []\n",
    "    topics = range(topic_dist.shape[1])\n",
    "    topic_labels = list('goss_' + str(each) for each in topics)\n",
    "\n",
    "    for user in range(topic_dist.shape[0]):  # each user\n",
    "        temp_goss = list(single_goss(topic_dist, user, topic)\n",
    "                         for topic in topics)  # calculate all GOSS scores per topic\n",
    "        goss.append(temp_goss)  # store all GOSS via nested lists\n",
    "\n",
    "    np_goss = np.array(goss)  # recast as np array..\n",
    "    return pd.DataFrame(goss, columns=topic_labels)  # and then to pandas df..\n",
    "\n",
    "\n",
    "def single_loss(topic_dist, i, k):\n",
    "    # calculate loss score for a particular user/topic (i/k) combination\n",
    "    # 1.0 return mu(xi) for specific user, sum topic probabilities, return\n",
    "    # average\n",
    "    mu_xi = np.sum(topic_dist[i, :]) / topic_dist.shape[1]\n",
    "\n",
    "    # 2.0 calculate muXI diff - GOSS equation numerator\n",
    "    loss_numerator = topic_dist[i, k] - mu_xi\n",
    "\n",
    "    # 3.0 for all topics (k) and a specific user (i):\n",
    "    # - sum the squared difference of all associated topic probabilities and mu(xi)\n",
    "    # - find the square of this sum\n",
    "    loss_denominator = 0\n",
    "    for user_prob in topic_dist[i, :]:\n",
    "        loss_denominator += (user_prob - mu_xi) ** 2\n",
    "\n",
    "    # 3.1 find sqrt of loss denominator\n",
    "    loss_denominator = sqrt(loss_denominator)\n",
    "\n",
    "    # 4.0 divide loss numerator by loss denominator to find loss score for\n",
    "    # specific user\n",
    "    return loss_numerator / loss_denominator\n",
    "\n",
    "\n",
    "def all_loss(topic_dist):\n",
    "    # calculate LOSS scores for a particular topic distribution\n",
    "    loss = []\n",
    "    topics = range(topic_dist.shape[1])\n",
    "    topic_labels = list('loss_' + str(each) for each in topics)\n",
    "\n",
    "    for user in range(topic_dist.shape[0]):  # each user\n",
    "        temp_loss = list(single_loss(topic_dist, user, topic)\n",
    "                         for topic in topics)  # calculate all loss scores per topic\n",
    "        # store all loss scores for each user via nested lists\n",
    "        loss.append(temp_loss)\n",
    "\n",
    "    np_loss = np.array(loss)  # cast to np array..\n",
    "    # and finally to pandas df..\n",
    "    return pd.DataFrame(np_loss, columns=topic_labels)\n",
    "\n",
    "\n",
    "def generate_dynamic_features(tf_matrix, params):\n",
    "    # 1. fit LDA model using term frequency matrix\n",
    "    lda = create_lda(tf_matrix, params)\n",
    "\n",
    "    # 2. generate document-topic distribution\n",
    "    dt_dist = create_dt_dist(lda, tf_matrix)\n",
    "\n",
    "    # 3. retrieve entropy for document-topic distribution\n",
    "    dt_entropy = entropy_all(dt_dist)\n",
    "\n",
    "    # 4. retrieve GOSS and LOSS scores\n",
    "    goss_df = all_goss(dt_dist)\n",
    "    loss_df = all_loss(dt_dist)\n",
    "\n",
    "    # 5. glue new features together into single df\n",
    "    dynamic_features = pd.concat([dt_entropy, goss_df, loss_df], axis=1)\n",
    "\n",
    "    # sanitize for possible NAN entries, possible bug withing GOSS/LOSS\n",
    "    # generator functions\n",
    "    dynamic_features = dynamic_features.dropna()\n",
    "\n",
    "    return dynamic_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
