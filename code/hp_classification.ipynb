{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing classification upon final dataframe/s for honeypot dataset..\n",
      "\n",
      "Executing Classification with the following params:\n",
      " {\n",
      "    \"classification_features\": [\n",
      "        {\n",
      "            \"about_me_length\": true, \n",
      "            \"dt_entropy\": true, \n",
      "            \"goss_0\": true, \n",
      "            \"goss_1\": true, \n",
      "            \"goss_2\": true, \n",
      "            \"goss_3\": true, \n",
      "            \"goss_4\": true, \n",
      "            \"loss_0\": true, \n",
      "            \"loss_1\": true, \n",
      "            \"loss_2\": true, \n",
      "            \"loss_3\": true, \n",
      "            \"loss_4\": true, \n",
      "            \"num_annotations\": true, \n",
      "            \"num_followers\": true, \n",
      "            \"num_followings\": true, \n",
      "            \"num_http\": true, \n",
      "            \"num_tweets\": true, \n",
      "            \"num_unique_words\": true, \n",
      "            \"tweet_avg_length\": true, \n",
      "            \"user_name_length\": true, \n",
      "            \"user_type\": true\n",
      "        }, \n",
      "        {\n",
      "            \"about_me_length\": true, \n",
      "            \"dt_entropy\": true, \n",
      "            \"goss_0\": true, \n",
      "            \"goss_1\": true, \n",
      "            \"goss_2\": true, \n",
      "            \"goss_3\": true, \n",
      "            \"goss_4\": true, \n",
      "            \"loss_0\": true, \n",
      "            \"loss_1\": true, \n",
      "            \"loss_2\": true, \n",
      "            \"loss_3\": true, \n",
      "            \"loss_4\": true, \n",
      "            \"num_annotations\": true, \n",
      "            \"num_followers\": true, \n",
      "            \"num_followings\": true, \n",
      "            \"num_http\": true, \n",
      "            \"num_tweets\": true, \n",
      "            \"num_unique_words\": true, \n",
      "            \"tweet_avg_length\": true, \n",
      "            \"user_name_length\": true, \n",
      "            \"user_type\": true\n",
      "        }, \n",
      "        {\n",
      "            \"about_me_length\": true, \n",
      "            \"dt_entropy\": true, \n",
      "            \"goss_0\": true, \n",
      "            \"goss_1\": true, \n",
      "            \"goss_2\": true, \n",
      "            \"goss_3\": true, \n",
      "            \"goss_4\": true, \n",
      "            \"loss_0\": true, \n",
      "            \"loss_1\": true, \n",
      "            \"loss_2\": true, \n",
      "            \"loss_3\": true, \n",
      "            \"loss_4\": true, \n",
      "            \"num_annotations\": true, \n",
      "            \"num_followers\": true, \n",
      "            \"num_followings\": true, \n",
      "            \"num_http\": true, \n",
      "            \"num_tweets\": true, \n",
      "            \"num_unique_words\": true, \n",
      "            \"tweet_avg_length\": true, \n",
      "            \"user_name_length\": true, \n",
      "            \"user_type\": true\n",
      "        }\n",
      "    ], \n",
      "    \"classification_models\": {\n",
      "        \"adaboosted_dt\": true, \n",
      "        \"decision_tree\": true, \n",
      "        \"linear_svc\": true, \n",
      "        \"random_forest\": true\n",
      "    }, \n",
      "    \"k_fold\": {\n",
      "        \"folds\": 10, \n",
      "        \"train\": 0.6\n",
      "    }\n",
      "}\n",
      "Generating models for cluster0.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KARTHIK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decision_tree Accuracy: 0.85 (+/- 0.02)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.83      0.79      0.81       403\\n          1       0.87      0.90      0.89       639\\n\\navg / total       0.86      0.86      0.86      1042\\n')\n",
      "('Confusion matrix:\\n', array([[319,  84],\n",
      "       [ 65, 574]], dtype=int64))\n",
      "adaboosted_dt Accuracy: 0.89 (+/- 0.02)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.87      0.84      0.86       403\\n          1       0.90      0.92      0.91       639\\n\\navg / total       0.89      0.89      0.89      1042\\n')\n",
      "('Confusion matrix:\\n', array([[340,  63],\n",
      "       [ 51, 588]], dtype=int64))\n",
      "linear_svc Accuracy: 0.81 (+/- 0.19)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.89      0.80      0.84       403\\n          1       0.88      0.94      0.91       639\\n\\navg / total       0.89      0.89      0.88      1042\\n')\n",
      "('Confusion matrix:\\n', array([[323,  80],\n",
      "       [ 39, 600]], dtype=int64))\n",
      "random_forest Accuracy: 0.90 (+/- 0.01)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.87      0.85      0.86       403\\n          1       0.91      0.92      0.91       639\\n\\navg / total       0.89      0.89      0.89      1042\\n')\n",
      "('Confusion matrix:\\n', array([[344,  59],\n",
      "       [ 53, 586]], dtype=int64))\n",
      "Generating models for cluster1.csv\n",
      "decision_tree Accuracy: 0.88 (+/- 0.02)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.91      0.90      0.91       645\\n          1       0.80      0.81      0.81       310\\n\\navg / total       0.87      0.87      0.87       955\\n')\n",
      "('Confusion matrix:\\n', array([[583,  62],\n",
      "       [ 59, 251]], dtype=int64))\n",
      "adaboosted_dt Accuracy: 0.91 (+/- 0.01)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.91      0.94      0.93       645\\n          1       0.87      0.81      0.84       310\\n\\navg / total       0.90      0.90      0.90       955\\n')\n",
      "('Confusion matrix:\\n', array([[607,  38],\n",
      "       [ 60, 250]], dtype=int64))\n",
      "linear_svc Accuracy: 0.78 (+/- 0.15)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.96      0.86      0.91       645\\n          1       0.76      0.92      0.83       310\\n\\navg / total       0.89      0.88      0.88       955\\n')\n",
      "('Confusion matrix:\\n', array([[556,  89],\n",
      "       [ 25, 285]], dtype=int64))\n",
      "random_forest Accuracy: 0.91 (+/- 0.02)\n",
      "('Classification report:\\n', u'             precision    recall  f1-score   support\\n\\n          0       0.92      0.95      0.93       645\\n          1       0.88      0.83      0.86       310\\n\\navg / total       0.91      0.91      0.91       955\\n')\n",
      "('Confusion matrix:\\n', array([[611,  34],\n",
      "       [ 52, 258]], dtype=int64))\n",
      "\n",
      "Classifications completed in 23.3999998569 seconds.\n",
      " Output saved to classification_results.json\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Classification Overview\n",
    "1. import all cluster-segregated dataframes\n",
    "2. for each dataframe, generate specified models using specified features\n",
    "3. for each group of models, evaluate the accuracy of the models\n",
    "4. concatenate results into single dictionary object, export object\n",
    "\n",
    "Model Evaluation Notes\n",
    "Each model is evaluated using kfold cross validation and other\n",
    "performance metrics (confusion matrix, precision, recall, f1 and support\n",
    "scores).\n",
    "\n",
    "To keep the output of this analysis manageable, similar yet distinct\n",
    "splitting operations are performed upon the data. For KFold CV, the\n",
    "data is split into x/y components. Cross fold validation is performed\n",
    "via an sklearn shuffle-split CV iterator, upon which an averaged\n",
    "accuracy score is obtained over the sequence of CV outcomes.\n",
    "\n",
    "The latter evaluations (conf. matrix etc.) are derived from a model\n",
    "fitted with the data once (as opposed to multiple fits, as is the\n",
    "case with CV). To facilitate this, the data is split once into\n",
    "x/y, train/test portions and fitted/analysed. There is therefore\n",
    "a difference in the way the accuracy derived and the way the latter\n",
    "analysis is derived. A stratified and shuffled mechanism is used to\n",
    "split the data for the lattter evaluations, in an attempt to replicate\n",
    "the mechanism utilized by the CV iterator.\n",
    "\n",
    "Classification ToDo\n",
    "Visualize model performance => export visualizations as additional outputs\n",
    "'''\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC  # or linearSVC\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, classification_report\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.insert(0, 'C:/Users/KARTHIK/Desktop/spam detection using clusering and classification/code/scripts/util')\n",
    "import util\n",
    "\n",
    "# define IO directories and files\n",
    "dirs = {'cluster_frames': 'cluster_frames',\n",
    "        'param_import': 'hp_classification_config.json',\n",
    "        'report_output': 'classification_results.json'}\n",
    "\n",
    "\n",
    "def extract_models(model_dict):\n",
    "    # return dictionary of models based upon user specification\n",
    "    selected_models = {'decision_tree': DecisionTreeClassifier(),  # prepopulate model options\n",
    "                       'random_forest': RandomForestClassifier(),\n",
    "                       'adaboosted_dt': AdaBoostClassifier(),\n",
    "                       'linear_svc': LinearSVC()}\n",
    "\n",
    "    for key, val in model_dict.items():  # filter based upon config input\n",
    "        if not (val):\n",
    "            del selected_models[key]\n",
    "\n",
    "    return selected_models\n",
    "\n",
    "\n",
    "def course_split(df):\n",
    "    # entire x/y partition for cross fold validation process\n",
    "    return df.drop(['user_type'], axis=1), df['user_type']\n",
    "\n",
    "\n",
    "def fine_split(df):\n",
    "    # x/y train/test splits, required for one time fitting\n",
    "    y = df['user_type']\n",
    "    X = df.drop(['user_type'], axis=1)\n",
    "    X_mat = X.as_matrix().astype(np.float)\n",
    "    return train_test_split(X_mat, y, test_size=0.4, random_state=42, stratify=y, shuffle=True)\n",
    "\n",
    "\n",
    "def kfold(model, model_name, X, y):\n",
    "    # define cv iterator parameters\n",
    "    cv = StratifiedShuffleSplit(n_splits=10, test_size=0.4, random_state=42)\n",
    "    scores = cross_val_score(model, X, y, cv=cv)\n",
    "\n",
    "    kfold_results = {'averaged_accuracy_score': '{0:.2f}'.format(scores.mean()),\n",
    "                     'averaged_accuracy_sd': '{0:.2f}'.format(scores.std() * 2)}\n",
    "\n",
    "    # retrieve trained model accuracy using cross fold validation score -\n",
    "    # using all data\n",
    "    print(\"{0} Accuracy: {1:.2f} (+/- {2:.2f})\".format(model_name,\n",
    "                                                       scores.mean(), scores.std() * 2))\n",
    "    return kfold_results\n",
    "\n",
    "# evaluate models performance using classification report and confusion matrix\n",
    "\n",
    "\n",
    "def metrics(model, X_train, X_test, y_train, y_test):\n",
    "    # classification report and confusion matrix - using train/test partitions\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metric_results = defaultdict(dict)\n",
    "    class_report = util.format_class_report(\n",
    "        precision_recall_fscore_support(y_test, y_pred))\n",
    "    metric_results = {'classification_report': class_report,\n",
    "                      'confusion_matrix': confusion_matrix(y_test, y_pred).tolist()}\n",
    "\n",
    "    print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    return metric_results    \n",
    "\n",
    "\n",
    "\n",
    "def generate_models_single(df, df_name, models, results_dict):\n",
    "    print('Generating models for {}'.format(df_name))\n",
    "    # intermediate results object for single frame\n",
    "    temp_dict = defaultdict(dict)\n",
    "\n",
    "    # partion dataframe\n",
    "    X, y = course_split(df)\n",
    "    X_train, X_test, y_train, y_test = fine_split(df)\n",
    "\n",
    "    # evaluate all models, append results\n",
    "    for model_name, model in models.items():\n",
    "        st = time.time()\n",
    "\n",
    "        # accuracy score based upon multiple model accuracy scores => kfold\n",
    "        # validation\n",
    "        temp_dict[model_name]['kfold_scores'] = kfold(model, model_name, X, y)\n",
    "        # scores based upon a single model fitting\n",
    "        temp_dict[model_name]['accuracy_metrics'] = metrics(\n",
    "            model, X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        #    confusion(model,X_train, X_test, y_train, y_test)\n",
    "\n",
    "        # record time taken to fit/evaluate each model\n",
    "        et = time.time() - st\n",
    "        temp_dict[model_name]['time_elapsed'] = '{0:.2f}'.format(et)\n",
    "\n",
    "    # append to main results object\n",
    "    results_dict[df_name] = temp_dict\n",
    "\n",
    "\n",
    "def generate_models_all(df_list, df_names, params, results_object):\n",
    "    # generate models for a list of dataframes\n",
    "\n",
    "    # configure model listing\n",
    "    models = extract_models(params['classification_models'])\n",
    "\n",
    "    for df, df_name, feature_dict in zip(df_list, df_names, params['classification_features']):\n",
    "        # configure features for each dataframe\n",
    "        features = util.extract_features(feature_dict)\n",
    "        df = util.choose_features(df, features)\n",
    "        df['user_name_length'].fillna(0,inplace=True)\n",
    "        df['tweet_avg_length'].fillna(0,inplace=True)\n",
    "        df['num_annotations'].fillna(0,inplace=True)\n",
    "        df['num_followers'].fillna(0,inplace=True)\n",
    "        df['num_http'].fillna(0,inplace=True)\n",
    "        df['num_unique_words'].fillna(0,inplace=True)\n",
    "        df['about_me_length'].fillna(0,inplace=True)\n",
    "        df['num_followings'].fillna(0,inplace=True)\n",
    "        df.to_csv(r'C:\\Users\\KARTHIK\\Desktop\\testing1.csv')\n",
    "        generate_models_single(df, df_name, models, results_object)\n",
    "\n",
    "\n",
    "def main():\n",
    "    st = time.time()\n",
    "    print('\\nPerforming classification upon final dataframe/s for honeypot dataset..\\n')\n",
    "    params = util.parse_params(dirs['param_import'], 'Classification')\n",
    "    results = defaultdict(dict)\n",
    "\n",
    "    # 1. import all dataframes\n",
    "    file_names = util.retrieve_files(\n",
    "        dirs['cluster_frames'])\n",
    "    all_frames = util.import_frames(file_names)\n",
    "\n",
    "    # 2. generate all models\n",
    "    generate_models_all(all_frames, util.trim_file_paths(\n",
    "        file_names), params, results)\n",
    "\n",
    "    et = time.time() - st\n",
    "    results['total_time_elapsed'] = '{0:.2f}'.format(et)\n",
    "\n",
    "    # 3. export all results\n",
    "    util.export_results(dirs['report_output'], results)\n",
    "    print('\\nClassifications completed in {0} seconds.\\n Output saved to {1}'.format(\n",
    "        et, dirs['report_output']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
