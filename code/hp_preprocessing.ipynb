{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing static and dynamic features..\n",
      "\n",
      "Executing Preprocessing with the following params:\n",
      " {\n",
      "    \"random_sub_sample\": true, \n",
      "    \"sample_size\": 1000\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KARTHIK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:128: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "C:\\Users\\KARTHIK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\KARTHIK\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing completed in 52.4820001125 seconds. Preprocessed files saved to:\n",
      "\n",
      " static_features.csv\n",
      "dynamic_features_intermediate.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Preprocessing High Level Overview\n",
    "1. import raw data sets\n",
    "2. preprocess static features and dynamic features (tweet corpus) dataframes\n",
    "3. square dataframes; ensure all static features user_id's exist within tweet corpus and vice versa\n",
    "(ensures a '1:many' relationship exists for user:tweets)\n",
    "4. export finalized static features, intermediate dynamic features dataframes\n",
    "\n",
    "Preprocessing Todo\n",
    "Declare column datatypes upon import => optimize storage (eg. use int instead of floats)\n",
    "Consolidate raw, static features as csv file instead of arff format \n",
    "'''\n",
    "\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "sys.path.insert(0, 'C:/Users/KARTHIK/Desktop/spam detection using clusering and classification/code/scripts/util')\n",
    "import util\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define import/export directories\n",
    "dirs = {'static_import': 'FinalDataFull.csv.arff',\n",
    "        'static_export': 'static_features.csv',\n",
    "        'dynamic_imports': ['cp_tweets.csv','lu_tweets.csv'],\n",
    "        'dynamic_export': 'dynamic_features_intermediate.csv',\n",
    "        'param_import': 'hp_preprocessing_config.json', }\n",
    "\n",
    "# rename dataframe columns\n",
    "static_rename_dict = {'UserID': 'user_id', 'UserType': 'user_type', 'NumberOfTweets': 'num_tweets', 'numOfFollowers': 'num_followers',\n",
    "                      'numOfFollowings': 'num_followings', 'lengthAboutMe': 'about_me_length', 'lengthUsername': 'user_name_length',\n",
    "                      'NumOfAnnotation': 'num_annotations', 'NumOfHttp': 'num_http', 'avgLengthOfTweets': 'tweet_avg_length',\n",
    "                      'totalNumOfUniqWords': 'num_unique_words'}\n",
    "dynamic_rename_dict = {'UserID': 'user_id', 'TweetID': 'tweet_id','Tweet': 'tweet', 'CreatedAt': 'creation_date'}\n",
    "\n",
    "\n",
    "def preprocess_static_features(import_path):\n",
    "    data = arff.loadarff(import_path)\n",
    "    df = pd.DataFrame(data[0])\n",
    "\n",
    "    # rename columns headers\n",
    "    df.rename(columns=static_rename_dict, inplace=True)\n",
    "\n",
    "    # correct for usertype boolean type\n",
    "    #df['user_type'] = df['user_type'].astype(int)\n",
    "\n",
    "    # drop any duplicate entries\n",
    "    return df.drop_duplicates(['user_id'])\n",
    "\n",
    "\n",
    "def preprocess_dynamic_features(import_paths):\n",
    "    cp_tweets = preprocess_tweet_set(import_paths[0])\n",
    "    lu_tweets = preprocess_tweet_set(import_paths[1])\n",
    "\n",
    "    # ensure cp and lu tweet sets are complimentary\n",
    "    cp_tweets_set = cp_tweets.loc[~cp_tweets['user_id'].isin(\n",
    "        lu_tweets['user_id'])]  # negated match\n",
    "    lu_tweets_set = lu_tweets.loc[~lu_tweets[\n",
    "        'user_id'].isin(cp_tweets['user_id'])]\n",
    "\n",
    "    # flag as illegitimate/legitimate users\n",
    "    cp_tweets_set['user_type'] = 1\n",
    "    lu_tweets_set['user_type'] = 0\n",
    "\n",
    "    # merge data frames\n",
    "    return pd.concat([cp_tweets_set, lu_tweets_set])\n",
    "\n",
    "\n",
    "def preprocess_tweet_set(tweets_path):\n",
    "    tweets = util.import_frame(tweets_path)\n",
    "\n",
    "    # rename columns headers\n",
    "    tweets.rename(columns=dynamic_rename_dict, inplace=True)\n",
    "\n",
    "    # variable removal\n",
    "    tweets = tweets.drop(['creation_date'],\n",
    "                         axis=1)\n",
    "\n",
    "    # flag NAN entries in UserID column\n",
    "    tweets['user_id'] = pd.to_numeric(tweets['user_id'], errors='coerce')\n",
    "\n",
    "    # remove row entries with malformed UserID\n",
    "    tweets = tweets.dropna(subset=['user_id'])\n",
    "\n",
    "    # tweet collating, group by user_id\n",
    "    tweets = tweets.groupby(['user_id'])['tweet'].apply(\n",
    "        list)\n",
    "    \n",
    "    #print(type(tweets))\n",
    "    #print(tweets.head(1))\n",
    "    # join tweets together into single document\n",
    "    #tweetJoin = lambda x: ' '.join(x)\n",
    "    #tweets = tweets.apply(tweetJoin)\n",
    "\n",
    "    # dataframe reformatting\n",
    "    tweets = tweets.to_frame()  # cast back to frame\n",
    "    tweets = tweets.reset_index()  # reset/adjust index\n",
    "\n",
    "    return tweets\n",
    "\n",
    "\n",
    "def square_frames(df_a, df_b, params):\n",
    "    # join two dataframes based upon user_id\n",
    "    df_a_set = df_a.loc[df_a['user_id'].isin(\n",
    "        df_b['user_id'])]  # ensure for match\n",
    "    df_b_set = df_b.loc[df_b['user_id'].isin(df_a['user_id'])]\n",
    "\n",
    "    # return small sample for prototyping speed, if specified\n",
    "    if (params['random_sub_sample']):\n",
    "        df_a_set = df_a_set.sample(\n",
    "            n=params['sample_size'], axis=0, random_state=42)\n",
    "        df_b_set = df_b_set.loc[df_b['user_id'].isin(\n",
    "            df_a_set['user_id'])]  # match user type\n",
    "\n",
    "    return df_a_set, df_b_set  # retain original frame housing\n",
    "\n",
    "\n",
    "def main():\n",
    "    st = time.time()\n",
    "    print('\\nPreprocessing static and dynamic features..\\n')\n",
    "    params = util.parse_params(dirs['param_import'], 'Preprocessing')\n",
    "\n",
    "    # 1. preprocess static and dynamic dataframes\n",
    "    static_df = preprocess_static_features(dirs['static_import'])\n",
    "    dynamic_df = preprocess_dynamic_features(dirs['dynamic_imports'])\n",
    "\n",
    "    # 2. square dataframes; ensure 1:many user:tweets relationship\n",
    "    static_df, dynamic_df = square_frames(static_df, dynamic_df, params)\n",
    "    util.export_frames([static_df, dynamic_df], [dirs['static_export'],\n",
    "                                                 dirs['dynamic_export']])  # 3. export dataframes\n",
    "\n",
    "    et = time.time() - st\n",
    "    print('\\nPreprocessing completed in {0} seconds. Preprocessed files saved to:\\n\\n {1}\\n{2}'.format(\n",
    "        et, dirs['static_export'], dirs['dynamic_export']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
